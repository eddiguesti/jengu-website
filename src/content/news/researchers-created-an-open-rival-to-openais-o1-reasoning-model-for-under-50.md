---
title: "Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50"
subtitle: "An insightful look into 'Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50'"
description: "In an impressive feat of cost-effective AI development, researchers from Stanford and the University of Washington have unveiled an AI \"reasoning\" model called s1, which was trained for under $50 in cloud credits. Demonstrating performance akin to high-end models from OpenAI and DeepSeek, s1 was distilled from Google’s Gemini 2.0 Flash Thinking Experimental, using a small dataset of precisely curated questions and answers. This breakthrough challenges the notion that significant financial resources are essential for innovation in AI, hinting at the potential commoditization of AI model development. As big tech firms like Meta, Google, and Microsoft plan massive investments in AI infrastructure, the modest success of s1 underscores the effectiveness of distillation methods for recreating existing model capabilities"
publishedOn: 2025-04-07
updatedOn: 2025-02-17
mainImage: "https://cdn.prod.website-files.com/672916ba596e9ad764eea6e4/67a634a1fe543829d9897985_tmpmvsaaui1.png"
thumbnail: ""
altTextThumbnail: ""
altTextMainImage: "A visually stunning main image for the article: Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50"
source: "techcrunch.com"
newsDate: 2025-02-07
draft: false
archived: false
---

```html
<h2>Researchers Develop Affordable Open-Source AI 'Reasoning' Model</h2>

<h3>Introduction</h3>
<p>In a groundbreaking development, AI researchers from Stanford and the University of Washington have successfully trained an alternative AI ‘reasoning’ model to rival OpenAI’s o1. Remarkably, the project was executed with a budget of less than $50 in cloud compute credits, a significant reduction from the multi-million-dollar expenditures that typically mark such projects. The researchers unveiled their findings in a paper released last Friday.</p>

<h3>About the s1 Model</h3>

<h4>Performance and Availability</h4>
<p>Named s1, this model matches the performance of leading reasoning models such as OpenAI’s o1 and DeepSeek’s R1 in tasks related to mathematics and coding. The model, including its training data and code, is accessible on GitHub, allowing widespread use and collaboration.</p>

<h4>Innovative Approach to Training</h4>
<p>The s1 model builds on a foundational off-the-shelf AI model, which was fine-tuned using a method known as distillation. This process involves training the model on the responses of another AI, thereby inheriting its reasoning capabilities. In this case, s1 was distilled from Google's Gemini 2.0 Flash Thinking Experimental model, utilizing a strategy parallel to one recently employed by Berkeley researchers to develop a similar AI model.</p>

<h3>Implications for AI Development</h3>

<h4>Challenges to AI Commoditization</h4>
<p>This achievement raises questions about the accessibility and commoditization of AI technology. The ability to replicate sophisticated AI models at a fraction of their original cost diminishes barriers to entry for innovators and could disrupt the competitive landscape.</p>

<h4>Responses from Major AI Labs</h4>
<p>The success of s1 has elicited varied responses, with some major AI labs expressing dissatisfaction. OpenAI, for instance, has accused DeepSeek of misusing its API data for distillation purposes. This tension highlights the ongoing debate around data use and model development in the industry.</p>

<h3>Technical Insights and Methodology</h3>

<h4>Fine-Tuning and Dataset Creation</h4>
<p>The s1 model was developed through supervised fine-tuning (SFT), which involves explicitly instructing an AI to emulate behaviors observed in a dataset. This technique proved to be more cost-effective than DeepSeek's broader reinforcement learning approach. For training, researchers compiled a dataset of 1,000 carefully chosen questions and answers, incorporating the reasoning process from Gemini 2.0.</p>

<h4>Computational Efficiency</h4>
<p>The training process for s1 was exceptionally efficient, taking just under 30 minutes with the aid of 16 Nvidia H100 GPUs. Niklas Muennighoff, one of the project's key researchers, noted that the necessary computational resources could be rented for about $20 today.</p>

<h4>Innovative Techniques</h4>
<p>One intriguing technique implemented during s1's development was instructing the model to "wait" during its reasoning process, resulting in improved accuracy. This simple yet effective method underscores the potential for innovative thinking in AI training.</p>

<h3>Future Prospects and Industry Impact</h3>

<p>As elite tech companies like Meta, Google, and Microsoft invest heavily in AI infrastructure to pioneer future AI models, methods like distillation present a cost-effective alternative for enhancing existing models. While these techniques offer significant savings, they do not yet herald revolutionary advancements. Nonetheless, the development of s1 illustrates the evolving dynamics of AI innovation, fostering both excitement and contention within the field.</p>
```