---
title: "Mistral Small 3 Language Model Achieves 81% MMLU Under Apache 2.0"
subtitle: "An insightful look into 'Mistral Small 3 Language Model Achieves 81% MMLU Under Apache 2.0'"
description: "Mistral AI has unveiled Mistral Small 3, a groundbreaking 24-billion-parameter language model available under the open-source Apache 2.0 license, achieving an impressive 81% accuracy on the MMLU benchmark. This latency-optimized model stands as a formidable alternative to larger and proprietary models like GPT4o-mini, delivering more than triple the speed of comparable models on identical hardware. Mistral Small 3 excels in diverse applications, from real-time conversational assistance and low-latency function calling to specialized fine-tuning for subject matter expertise in fields such as healthcare and finance. Designed for local deployment, it remains highly efficient even on consumer-grade setups, and is readily available on platforms like Hugging Face, Ollama"
publishedOn: 2025-04-07
updatedOn: 2025-02-06
mainImage: "https://cdn.prod.website-files.com/672916ba596e9ad764eea6e4/67a001485a14995d76f745d5_tmprcnb16nd.png"
thumbnail: ""
altTextThumbnail: ""
altTextMainImage: "A visually stunning main image for the article: Mistral Small 3 Language Model Achieves 81% MMLU Under Apache 2.0"
source: "mistral.ai"
newsDate: 2025-02-03
draft: false
archived: false
---

<h2>Mistral AI Unveils Mistral Small 3: A Pioneering Language Model</h2>

<h3>Introduction to Mistral Small 3</h3>

On January 30, 2025, the Mistral AI team proudly announced the release of Mistral Small 3, a groundbreaking 24 billion-parameter language model. Developed with a focus on latency optimization, the model is launched under the Apache 2.0 license, showcasing an impressive 81% accuracy in the Multi-task Language Understanding (MMLU) benchmark while achieving a processing speed of 150 tokens per second.

<h3>Competitive Edge and Industry Applications</h3>

Mistral Small 3 asserts its competitive stance by challenging larger models, such as Llama 3.3 70B and Qwen 32B, as well as proprietary alternatives like GPT4o-mini. With its advantageous size and performance, it matches the instruction-following capabilities of Llama 3.3 70B while operating over three times faster on equivalent hardware. The model is designed to deliver robust language and instruction processing for 80% of generative AI tasks that demand low latency, positioning it as a cost-effective solution for local deployment. 

<h4>Performance Benchmarks and Evaluation</h4>

Mistral AI conducted extensive human evaluations, collaborating with an external vendor to assess the model's performance on over 1,000 coding and generalist prompts. Although some differences were noted between internal and publicly available benchmarks, Mistral AI ensured the integrity of these evaluations. The instruction-tuned variant of Mistral Small 3 excels in key areas such as coding, mathematics, general knowledge, and instruction adherence, paralleling the performance of models three times its size.

<h3>Use Cases and Industry Adoption</h3>

Mistral Small 3 is being utilized across various sectors, exhibiting its versatility in several distinct use cases:

- **Fast-Response Conversational Assistance**: Ideal for virtual assistant applications requiring swift, accurate interactions.
- **Low-Latency Function Execution**: Beneficial for embedding in automated workflows that demand rapid command processing.
- **Fine-Tuning for Subject Matter Expertise**: Enhances performance in particular domains, such as legal, medical, and technical fields.
- **Local Inference**: A viable option for organizations with sensitive data requirements, operable on local hardware like RTX 4090 or MacBook with 32GB RAM.

Mistral Small 3 is being tested in various industries, from financial services for fraud detection, healthcare for patient triage, to robotics and automotive for command and control. 

<h4>Integration into Technology Stacks</h4>

Available on la Plateforme as "mistral-small-latest" or "mistral-small-2501", Mistral Small 3 is ready for integration into existing technology infrastructures. Mistral AI collaborates with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI, making the model accessible across their platforms. Additional support on NVIDIA NIM, Amazon SageMaker, Groq, Databricks, and Snowflake is forthcoming.

<h3>Future Directions and Open-Source Commitment</h3>

Looking ahead, Mistral AI aims to enhance the reasoning capabilities of its models, both small and large. The company maintains its commitment to open-source principles under the Apache 2.0 license, encouraging community engagement and collaboration. In addition to open-source models, Mistral AI plans to develop specialized commercial models tailored to increase speed, context understanding, and domain-specific knowledge, supporting a wide array of enterprise applications.

With Mistral Small 3, Mistral AI continues to push the boundaries of language model development, paving the way for accessible, efficient, and transformative AI applications.